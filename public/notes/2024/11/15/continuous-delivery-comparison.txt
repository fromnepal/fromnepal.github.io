compare these two 

first: 
how good is AI code generation really
there's clearly enormous commercial
potential for companies if AI can
generate good effective code more
efficiently ultimately at lower cost
than human programmers but can I really
do that clearly AI can generate code but
is it good and effective that's our
topic for today
hi I'm Dave Farley of continuous
delivery and welcome to my channel if
you haven't been here before please do
hit subscribe and if you enjoy the
content today hit like as well in this
episode I'm going to be asking for your
help but don't worry I think it'll also
be some fun some friends of mine are
conducting some interesting academic
research into AI assisted coding and are
looking for people to take part in their
experiment they're aim is to see if AI
systems can really help us to build high
quality code in the real world of
professional software development so do
watch for a chance to do something
interesting help with some research and
also win some prizes whatever you think
about machine Learning Systems writing
code whether or not they're useful tools
that help us to do a better job or
whether or not they're just all going to
replace all of our programming jobs next
week it seems certain that they change
the game
but it also seems pretty clear that we
don't really know how yet I don't think
that anyone really understands yet the
scale or nature of this change we may
guess or imagine what is coming but it
is certainly true that for the first
time we have something other than people
that can write code and we don't know
what that means where it will be helpful
and where it Won't Anyone who says that
they know how this will play out is only
guessing this stage we're extremely
fortunate on on the continuous delivery
channel to be sponsored by equal experts
transic topple honeycomb and Ultra edit
equal experts is a multinational
consultancy built on applying the ideas
and techniques that we talk about here
to build great software for their
clients transic is a financial
technology company applying Advanced
continuous delivery techniques to
deliver low latency trade rout services
to some of the biggest financial
institutions in the world topple build
software to make pair programming easier
for people who work remotely honeycom
help engineering teams deeply understand
their own production systems through
observability and Ultra edit is a
powerful configurable text editor
capable of hex and text code editing
which boasts on rivaled performance
particularly handling large files all
these company offer products and
services that are very very well aligned
with the topics that we discuss on this
channel every week so if you're looking
for excellence in continuous delivery
and software engineering in general do
click on the links in the description
below and check them out one things that
seems pretty clear to me at least for
the current generation of large language
models is that they write code in a
profoundly different way to the way that
human beings write code and this is
different in ways that matter maybe even
limiting to how far the help the AI
assisted coding can take us the most
profound change as far as I can see is
the loss of
reproducibility if we ask a large
language model to generate some code for
us it will do so and it may or may not
be good code but if we ask the same
large language model in precisely the
same words to write the same code again
it won't generate the same code again
it'll provide a new implementation to
meet our needs and that too may be good
or may be bad but there's nothing to say
that the bits that were good before are
still good in this new version because
we aren't enhancing what's already there
before the AI is rewriting everything
from scratch if we use the AI assistance
this way this profoundly changes our
relationship with the code it's also
very similar to the way that many past
attempts at making programming easier
have failed in the past because it
misses something really important about
software development there's a danger
that we rely on the rather naive
assumption that code creation or
generation happens once and is then over
leaving no room for refinement and
correction as we learn more about what
our users need and how best to solve the
problems that we are faced with in human
software development we've learned how
to deal with this kind of complexity
with ideas like Version Control giving
us the freedom to make mistakes and to
recover from it quickly easily and
cheaply when we do building high quality
code in systems is for humans at least
always an incremental process these days
mediated through Version Control complex
systems are built step by step verifying
things after each version controlled
step those steps may be big or small big
steps in the form of new releases over
years or even decades think of the
evolution of Windows or Mac OS version
by version or better we may use small
steps each one representing tiny
increments in the behavior of our system
think continuous delivery all this is
still an incremental process but
incrementalism like this relies on us
being able to reestablish a known
starting point and then modifying the
code to add new features or correct
broken ones if every change means
rebuilding the whole system from scratch
we even those of us who are machines
limit our ability to learn and build on
top of what we've already learned and so
improve as we go large language models
don't obviously allow for this and this
is a very big deal because as far as we
can tell this is the only way in which
you can learn and improve things in
human terms I would and do argue that
the defining characteristics for quality
in software is measured by our ability
to change it in large part that's
because it facilitates our ability to
grow and evolve our software
incrementally in this way high quality
software is easy to change lowquality
software is not it really is that simple
so does this mean that generative AI
helps or hinders in creating highquality
software on that basis some of my
friends that equal experts code scene
and the department of computer science
at lond University in Sweden agree that
this is an interesting question and have
designed a study to evaluate it previous
empirical studies of AI code assistants
have found that they offer substantial
increases in productivity but as we know
from practical Human Experience we can
easily misjudge this kind of thing by
looking at the wrong time scales when it
comes to productivity I may make
progress faster over the course of a few
days by cutting Corners not testing and
building crap software but over the
course of weeks and months or years the
cost of this strategy leads to us
building Big Balls of unmaintainable mud
and that can slow or even halt the
process of software development alt
together so it's certainly not a gain in
productivity the Dora metrics tell us
that for human-built software at least
we must build high quality software if
we want to move quickly so does AI
assistance help us to keep moving fast
over longer periods of time does it help
us to build more maintainable code
that's the question that we'd like to
answer in this study the study will
operate in two phases with three
different groups of people and we are
looking for people for each group the
basic idea is that maintainable code is
easy to reason about and change by
someone other than the original author
so that is what we aim to test we start
with some tricky poor buggy code and the
subjects of the experiment are then
tasked with modifying it
these subjects hopefully you are formed
into two groups one group is asked to
complete the task with the help of AI
code assistance and the other without
GitHub conducted a study of the use of
co-pilot with 95 professional
programmers tased with implementing an
HTTP server in JavaScript a little while
ago this study found that AI assistance
made the developers in the group that
use the AI assistance 55. 8% faster on
average when completing this task than
developers working without the aid of
co-pilot our research is aimed to look
at the to me more important measure of
the maintain maintainability of the
output rather than short-term
development performance which as I've
already suggested is a very poor measure
of software development in the real
world because if the output of AI
assisted code is easier to maintain then
we can be more confident that it really
is helping us to improve our throughput
but if it's not easier to maintain then
AI assisted code will ultimately be more
costly in the long run than working
without it in phase two of the study we
will evaluate the maintainability of the
output from each of the groups in Phase
One people in The Phase 2 group will
take the output of the coding Challenge
from phase one not knowing if the output
was generated with or without AI
assistance
and then they will be asked to complete
some new tasks to modify this code the
study will determine which is easier to
modify the AI assisted code or the not
AI assisted code the code will be
evaluated based on several different
metrics including code scenes very well
regarded measures of code Health time to
complete the task and the perceived
productivity of the people working on it
based on Nicole forren space framework
the problem itself is a fun real world
simulation of the kind of things that
human programmers do all the time the
code is complex enough to be interesting
more complicated than a simple coding
Carter for example but simple enough to
be able to understand and do useful work
within just a few hours for disclosure
here I should say that my name is on
this study but the people at lond
University equal experts and code scene
have done all of the work I hope that
you'll join us and give us a few hours
of your time we expect the exercise will
take you 2 to 4 hours to complete if
you're interested in signing up there's
a link in the description below or you
could use this QR code both link to a
short survey that will help the
researchers to evaluate your fit as a
subject for the study and allocate you
to an appropriate group your code and
data that the researchers will collect
will be completely Anonymous and
everyone who completes the exercise as a
subject will win a prize including some
signed books from and me but of course
the most rewarding thing of all is that
everyone who takes part will be adding
to our understanding and knowledge of
what AI code assistants can really do
for us as developers so please do sign
up and try and fix the poor code that
the researchers have tried so hard to
make nasty enough to be ready for you
thank you very much for watching and if
you enjoy our stuff here on the
continuous delivery Channel then do
consider supporting us by joining our
patreon community and thanks once again
to our patrons who support our work here
thank you and bye-bye
[Music]

second: 
How good is AI code generation really? There's clearly enormous commercial potential for companies if AI can generate good, effective code more efficiently, ultimately at lower cost than human programmers. But can it really do that? Clearly, AI can generate code, but is it good and effective? That's our topic for today.

Hi, I'm Dave Farley of Continuous Delivery, and welcome to my channel. If you haven't been here before, please do hit subscribe, and if you enjoy the content today, hit like as well. In this episode, I'm going to be asking for your help, but don't worry, I think it'll also be some fun. Some friends of mine are conducting some interesting academic research into AI-assisted coding and are looking for people to take part in their experiment. Their aim is to see if AI systems can really help us to build high-quality code in the real world of professional software development. So do watch for a chance to do something interesting, help with some research, and also win some prizes.

Whatever you think about machine learning systems writing code, whether or not they're useful tools that help us to do a better job, or whether or not they're just all going to replace all of our programming jobs next week, it seems certain that they change the game. But it also seems pretty clear that we don't really know how yet. I don't think that anyone really understands yet the scale or nature of this change. We may guess or imagine what is coming, but it is certainly true that for the first time, we have something other than people that can write code, and we don't know what that means, where it will be helpful, and where it won't. Anyone who says that they know how this will play out is only guessing at this stage.

We're extremely fortunate on the Continuous Delivery channel to be sponsored by Equal Experts, Transic, Topple, Honeycomb, and UltraEdit. Equal Experts is a multinational consultancy built on applying the ideas and techniques that we talk about here to build great software for their clients. Transic is a financial technology company applying advanced continuous delivery techniques to deliver low-latency trade route services to some of the biggest financial institutions in the world. Topple builds software to make pair programming easier for people who work remotely. Honeycomb helps engineering teams deeply understand their own production systems through observability. And UltraEdit is a powerful, configurable text editor capable of hex and text code editing, which boasts unrivaled performance, particularly handling large files. All these companies offer products and services that are very, very well aligned with the topics that we discuss on this channel every week. So if you're looking for excellence in continuous delivery and software engineering in general, do click on the links in the description below and check them out.

One thing that seems pretty clear to me, at least for the current generation of large language models, is that they write code in a profoundly different way to the way that human beings write code. And this is different in ways that matter, maybe even limiting to how far the help the AI-assisted coding can take us. The most profound change, as far as I can see, is the loss of reproducibility. If we ask a large language model to generate some code for us, it will do so, and it may or may not be good code. But if we ask the same large language model in precisely the same words to write the same code again, it won't generate the same code again. It'll provide a new implementation to meet our needs, and that too may be good or may be bad. But there's nothing to say that the bits that were good before are still good in this new version because we aren't enhancing what's already there before. The AI is rewriting everything from scratch. If we use the AI assistance this way, this profoundly changes our relationship with the code.

It's also very similar to the way that many past attempts at making programming easier have failed in the past because it misses something really important about software development. There's a danger that we rely on the rather naive assumption that code creation or generation happens once and is then over, leaving no room for refinement and correction as we learn more about what our users need and how best to solve the problems that we are faced with. In human software development, we've learned how to deal with this kind of complexity with ideas like version control, giving us the freedom to make mistakes and to recover from them quickly, easily, and cheaply when we do. Building high-quality code in systems is, for humans at least, always an incremental process these days, mediated through version control. Complex systems are built step by step, verifying things after each version-controlled step. Those steps may be big or small. Big steps in the form of new releases over years or even decades—think of the evolution of Windows or Mac OS version by version. Or better, we may use small steps, each one representing tiny increments in the behavior of our system—think continuous delivery. All this is still an incremental process, but incrementalism like this relies on us being able to reestablish a known starting point and then modifying the code to add new features or correct broken ones. If every change means rebuilding the whole system from scratch, we—even those of us who are machines—limit our ability to learn and build on top of what we've already learned and so improve as we go. Large language models don't obviously allow for this, and this is a very big deal because, as far as we can tell, this is the only way in which you can learn and improve things in human terms.

I would and do argue that the defining characteristic for quality in software is measured by our ability to change it. In large part, that's because it facilitates our ability to grow and evolve our software incrementally. In this way, high-quality software is easy to change; low-quality software is not. It really is that simple. So does this mean that generative AI helps or hinders in creating high-quality software on that basis? Some of my friends at Equal Experts, CodeScene, and the Department of Computer Science at Lund University in Sweden agree that this is an interesting question and have designed a study to evaluate it.

Previous empirical studies of AI code assistants have found that they offer substantial increases in productivity. But as we know from practical human experience, we can easily misjudge this kind of thing by looking at the wrong time scales when it comes to productivity. I may make progress faster over the course of a few days by cutting corners, not testing, and building crap software. But over the course of weeks and months or years, the cost of this strategy leads to us building big balls of unmaintainable mud, and that can slow or even halt the process of software development altogether. So it's certainly not a gain in productivity. The DORA metrics tell us that for human-built software, at least, we must build high-quality software if we want to move quickly. So does AI assistance help us to keep moving fast over longer periods of time? Does it help us to build more maintainable code? That's the question that we'd like to answer in this study.

The study will operate in two phases with three different groups of people, and we are looking for people for each group. The basic idea is that maintainable code is easy to reason about and change by someone other than the original author. So that is what we aim to test. We start with some tricky, poor, buggy code, and the subjects of the experiment are then tasked with modifying it. These subjects, hopefully you, are formed into two groups. One group is asked to complete the task with the help of AI code assistance and the other without.

GitHub conducted a study of the use of Copilot with 95 professional programmers tasked with implementing an HTTP server in JavaScript a little while ago. This study found that AI assistance made the developers in the group that used the AI assistance 55.8% faster on average when completing this task than developers working without the aid of Copilot. Our research is aimed to look at the, to me, more important measure of the maintainability of the output rather than short-term development performance, which, as I've already suggested, is a very poor measure of software development in the real world. Because if the output of AI-assisted code is easier to maintain, then we can be more confident that it really is helping us to improve our throughput. But if it's not easier to maintain, then AI-assisted code will ultimately be more costly in the long run than working without it.

In phase two of the study, we will evaluate the maintainability of the output from each of the groups in phase one. People in the phase two group will take the output of the coding challenge from phase one, not knowing if the output was generated with or without AI assistance, and then they will be asked to complete some new tasks to modify this code. The study will determine which is easier to modify: the AI-assisted code or the non-AI-assisted code. The code will be evaluated based on several different metrics, including CodeScene's very well-regarded measures of code health, time to complete the task, and the perceived productivity of the people working on it based on Nicole Forsgren's SPACE framework.

The problem itself is a fun, real-world simulation of the kind of things that human programmers do all the time. The code is complex enough to be interesting—more complicated than a simple coding kata, for example—but simple enough to be able to understand and do useful work within just a few hours. For disclosure here, I should say that my name is on this study, but the people at Lund University, Equal Experts, and CodeScene have done all of the work. I hope that you'll join us and give us a few hours of your time. We expect the exercise will take you 2 to 4 hours to complete. If you're interested in signing up, there's a link in the description below, or you could use this QR code. Both link to a short survey that will help the researchers to evaluate your fit as a subject for the study and allocate you to an appropriate group. Your code and data that the researchers will collect will be completely anonymous, and everyone who completes the exercise as a subject will win a prize, including some signed books from me. But of course, the most rewarding thing of all is that everyone who takes part will be adding to our understanding and knowledge of what AI code assistants can really do for us as developers.

So please do sign up and try and fix the poor code that the researchers have tried so hard to make nasty enough to be ready for you. Thank you very much for watching, and if you enjoy our stuff here on the Continuous Delivery Channel, then do consider supporting us by joining our Patreon community. And thanks once again to our patrons who support our work here. Thank you and bye-bye.










Both texts discuss the potential and challenges of AI code generation, but there are some differences in their presentation and focus. Here's a comparison:

### Similarities:
1. **Topic**: Both texts explore the effectiveness and implications of AI-assisted code generation.
2. **Presenter**: Dave Farley, from the Continuous Delivery channel, is the speaker in both.
3. **Call to Action**: Both invite viewers to participate in a study on AI-assisted coding.
4. **Sponsorships**: Both mention sponsors like Equal Experts, Transic, Topple, Honeycomb, and UltraEdit.

### Differences:
1. **Length and Detail**:
   - **First Text**: Much longer and more detailed, covering various aspects of AI code generation, including reproducibility, incremental development, and the importance of maintainability.
   - **Second Text**: More concise, summarizing the main points without going into as much depth.

2. **Structure**:
   - **First Text**: Includes a detailed introduction, sponsorship mentions, an in-depth discussion on AI code generation, and a thorough explanation of the study.
   - **Second Text**: Follows a similar structure but is more streamlined and less repetitive.

3. **Tone and Style**:
   - **First Text**: More conversational and detailed, with a focus on explaining concepts thoroughly.
   - **Second Text**: More direct and to the point, making it easier to follow for those looking for a quick overview.

### Key Points Highlighted:
- **Reproducibility**: Both texts mention that AI-generated code lacks reproducibility, which is a significant difference from human-written code.
- **Incremental Development**: Both emphasize the importance of incremental development and how AI might impact this process.
- **Maintainability**: Both discuss the need to evaluate the maintainability of AI-generated code, with the first text providing more detailed insights.

### Conclusion:
The first text provides a comprehensive exploration of AI code generation, suitable for viewers who want an in-depth understanding. The second text offers a concise summary, ideal for those looking for a quick overview. Both effectively communicate the potential and challenges of AI-assisted coding and invite participation in the study to further understand its impact.










































what is missing in the second that is in the first? 

The second text is more concise and omits several details that are present in the first. Here are some key elements missing in the second text:

1. **Detailed Explanation of Reproducibility**:
   - The first text goes into depth about how AI-generated code lacks reproducibility, explaining that the same prompt can produce different code each time, which affects maintainability and the development process.

2. **Incremental Development**:
   - The first text discusses the importance of incremental development in software engineering and how AI-generated code might impact this process. It explains how human developers use version control to manage incremental changes, which is not straightforward with AI-generated code.

3. **Historical Context**:
   - The first text provides historical context by comparing AI code generation to past attempts at making programming easier, highlighting why those attempts failed and how similar issues might arise with AI.

4. **In-depth Study Design**:
   - The first text offers a more detailed description of the study being conducted, including the phases of the study, the tasks involved, and the metrics used to evaluate the maintainability of AI-generated code.

5. **Personal Insights and Opinions**:
   - The first text includes more of Dave Farley's personal insights and opinions on the topic, making it more conversational and engaging.

6. **Detailed Sponsorship Mentions**:
   - The first text provides more detailed descriptions of the sponsors and their relevance to the topic of continuous delivery and software engineering.

These additional details in the first text provide a richer, more comprehensive understanding of the topic, while the second text offers a more streamlined overview.