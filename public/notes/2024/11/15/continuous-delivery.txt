How good is AI code generation really? There's clearly enormous commercial potential for companies if AI can generate good, effective code more efficiently, ultimately at lower cost than human programmers. But can it really do that? Clearly, AI can generate code, but is it good and effective? That's our topic for today.

Hi, I'm Dave Farley of Continuous Delivery, and welcome to my channel. If you haven't been here before, please do hit subscribe, and if you enjoy the content today, hit like as well. In this episode, I'm going to be asking for your help, but don't worry, I think it'll also be some fun. Some friends of mine are conducting some interesting academic research into AI-assisted coding and are looking for people to take part in their experiment. Their aim is to see if AI systems can really help us to build high-quality code in the real world of professional software development. So do watch for a chance to do something interesting, help with some research, and also win some prizes.

Whatever you think about machine learning systems writing code, whether or not they're useful tools that help us to do a better job, or whether or not they're just all going to replace all of our programming jobs next week, it seems certain that they change the game. But it also seems pretty clear that we don't really know how yet. I don't think that anyone really understands yet the scale or nature of this change. We may guess or imagine what is coming, but it is certainly true that for the first time, we have something other than people that can write code, and we don't know what that means, where it will be helpful, and where it won't. Anyone who says that they know how this will play out is only guessing at this stage.

We're extremely fortunate on the Continuous Delivery channel to be sponsored by Equal Experts, Transic, Topple, Honeycomb, and UltraEdit. Equal Experts is a multinational consultancy built on applying the ideas and techniques that we talk about here to build great software for their clients. Transic is a financial technology company applying advanced continuous delivery techniques to deliver low-latency trade route services to some of the biggest financial institutions in the world. Topple builds software to make pair programming easier for people who work remotely. Honeycomb helps engineering teams deeply understand their own production systems through observability. And UltraEdit is a powerful, configurable text editor capable of hex and text code editing, which boasts unrivaled performance, particularly handling large files. All these companies offer products and services that are very, very well aligned with the topics that we discuss on this channel every week. So if you're looking for excellence in continuous delivery and software engineering in general, do click on the links in the description below and check them out.

One thing that seems pretty clear to me, at least for the current generation of large language models, is that they write code in a profoundly different way to the way that human beings write code. And this is different in ways that matter, maybe even limiting to how far the help the AI-assisted coding can take us. The most profound change, as far as I can see, is the loss of reproducibility. If we ask a large language model to generate some code for us, it will do so, and it may or may not be good code. But if we ask the same large language model in precisely the same words to write the same code again, it won't generate the same code again. It'll provide a new implementation to meet our needs, and that too may be good or may be bad. But there's nothing to say that the bits that were good before are still good in this new version because we aren't enhancing what's already there before. The AI is rewriting everything from scratch. If we use the AI assistance this way, this profoundly changes our relationship with the code.

It's also very similar to the way that many past attempts at making programming easier have failed in the past because it misses something really important about software development. There's a danger that we rely on the rather naive assumption that code creation or generation happens once and is then over, leaving no room for refinement and correction as we learn more about what our users need and how best to solve the problems that we are faced with. In human software development, we've learned how to deal with this kind of complexity with ideas like version control, giving us the freedom to make mistakes and to recover from them quickly, easily, and cheaply when we do. Building high-quality code in systems is, for humans at least, always an incremental process these days, mediated through version control. Complex systems are built step by step, verifying things after each version-controlled step. Those steps may be big or small. Big steps in the form of new releases over years or even decades—think of the evolution of Windows or Mac OS version by version. Or better, we may use small steps, each one representing tiny increments in the behavior of our system—think continuous delivery. All this is still an incremental process, but incrementalism like this relies on us being able to reestablish a known starting point and then modifying the code to add new features or correct broken ones. If every change means rebuilding the whole system from scratch, we—even those of us who are machines—limit our ability to learn and build on top of what we've already learned and so improve as we go. Large language models don't obviously allow for this, and this is a very big deal because, as far as we can tell, this is the only way in which you can learn and improve things in human terms.

I would and do argue that the defining characteristic for quality in software is measured by our ability to change it. In large part, that's because it facilitates our ability to grow and evolve our software incrementally. In this way, high-quality software is easy to change; low-quality software is not. It really is that simple. So does this mean that generative AI helps or hinders in creating high-quality software on that basis? Some of my friends at Equal Experts, CodeScene, and the Department of Computer Science at Lund University in Sweden agree that this is an interesting question and have designed a study to evaluate it.

Previous empirical studies of AI code assistants have found that they offer substantial increases in productivity. But as we know from practical human experience, we can easily misjudge this kind of thing by looking at the wrong time scales when it comes to productivity. I may make progress faster over the course of a few days by cutting corners, not testing, and building crap software. But over the course of weeks and months or years, the cost of this strategy leads to us building big balls of unmaintainable mud, and that can slow or even halt the process of software development altogether. So it's certainly not a gain in productivity. The DORA metrics tell us that for human-built software, at least, we must build high-quality software if we want to move quickly. So does AI assistance help us to keep moving fast over longer periods of time? Does it help us to build more maintainable code? That's the question that we'd like to answer in this study.

The study will operate in two phases with three different groups of people, and we are looking for people for each group. The basic idea is that maintainable code is easy to reason about and change by someone other than the original author. So that is what we aim to test. We start with some tricky, poor, buggy code, and the subjects of the experiment are then tasked with modifying it. These subjects, hopefully you, are formed into two groups. One group is asked to complete the task with the help of AI code assistance and the other without.

GitHub conducted a study of the use of Copilot with 95 professional programmers tasked with implementing an HTTP server in JavaScript a little while ago. This study found that AI assistance made the developers in the group that used the AI assistance 55.8% faster on average when completing this task than developers working without the aid of Copilot. Our research is aimed to look at the, to me, more important measure of the maintainability of the output rather than short-term development performance, which, as I've already suggested, is a very poor measure of software development in the real world. Because if the output of AI-assisted code is easier to maintain, then we can be more confident that it really is helping us to improve our throughput. But if it's not easier to maintain, then AI-assisted code will ultimately be more costly in the long run than working without it.

In phase two of the study, we will evaluate the maintainability of the output from each of the groups in phase one. People in the phase two group will take the output of the coding challenge from phase one, not knowing if the output was generated with or without AI assistance, and then they will be asked to complete some new tasks to modify this code. The study will determine which is easier to modify: the AI-assisted code or the non-AI-assisted code. The code will be evaluated based on several different metrics, including CodeScene's very well-regarded measures of code health, time to complete the task, and the perceived productivity of the people working on it based on Nicole Forsgren's SPACE framework.

The problem itself is a fun, real-world simulation of the kind of things that human programmers do all the time. The code is complex enough to be interesting—more complicated than a simple coding kata, for example—but simple enough to be able to understand and do useful work within just a few hours. For disclosure here, I should say that my name is on this study, but the people at Lund University, Equal Experts, and CodeScene have done all of the work. I hope that you'll join us and give us a few hours of your time. We expect the exercise will take you 2 to 4 hours to complete. If you're interested in signing up, there's a link in the description below, or you could use this QR code. Both link to a short survey that will help the researchers to evaluate your fit as a subject for the study and allocate you to an appropriate group. Your code and data that the researchers will collect will be completely anonymous, and everyone who completes the exercise as a subject will win a prize, including some signed books from me. But of course, the most rewarding thing of all is that everyone who takes part will be adding to our understanding and knowledge of what AI code assistants can really do for us as developers.

So please do sign up and try and fix the poor code that the researchers have tried so hard to make nasty enough to be ready for you. Thank you very much for watching, and if you enjoy our stuff here on the Continuous Delivery Channel, then do consider supporting us by joining our Patreon community. And thanks once again to our patrons who support our work here. Thank you and bye-bye.
